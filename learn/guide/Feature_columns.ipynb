{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature_column 分为三大类\n",
    "![title](img/feature_column.png)\n",
    "### Dense_Column\n",
    "### Categorial_Column\n",
    "### bucketized_column：继承上面两类，输出 one-hot\n",
    "\n",
    "### Dense_Column 可分为\n",
    "#### numerical_column: 整数\n",
    "##### indicator_column: one-hot\n",
    "##### embedding_column: 稠密矢量\n",
    "    \n",
    "### Categorial_Column 可分为\n",
    "##### categorical_column_with_identity: 数字分类后返回类别 index\n",
    "##### categorical_column_with_vocabulary_list：字符串分类后返回类别 index \n",
    "##### categorical_column_with_vocabulary_file：字符串分类后返回类别 index ，分类词汇存在文件\n",
    "##### categorical_column_with_hash_bucket：hash 后直接将字符串分成几类\n",
    "##### crossed_column：组合特征构造成整形特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考资料\n",
    "# https://tensorflow.google.cn/guide/feature_columns\n",
    "# https://www.jianshu.com/p/516e882699cf\n",
    "# https://www.jianshu.com/p/fceb64c790f3\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numeric_column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.],\n",
      "       [1.],\n",
      "       [2.],\n",
      "       [3.],\n",
      "       [4.],\n",
      "       [5.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 转化成数字\n",
    "# tf.feature_column.numeric_column(\n",
    "#     key,\n",
    "#     shape=(1,),\n",
    "#     default_value=None,\n",
    "#     dtype=tf.float32,\n",
    "#     normalizer_fn=None\n",
    "# )\n",
    "test = {'test': [[0.], [1.], [2.], [3.], [4.], [5.]]}\n",
    "column = tf.feature_column.numeric_column(key = 'test')\n",
    "tensor = tf.feature_column.input_layer(test,[column])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([tensor]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  bucketized_column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 分痛: 将数字分段后再转化为 one-hot\n",
    "# tf.feature_column.bucketized_column(\n",
    "#     source_column,\n",
    "#     boundaries\n",
    "# )\n",
    "year={\"year\":[1958, 1978, 1981, 1999, 2005,2010]}\n",
    "year_numeric = tf.feature_column.numeric_column(key ='year')\n",
    "# boundaries 界限\n",
    "year_bucket = tf.feature_column.bucketized_column(year_numeric,[1960,1980,2000,2006])\n",
    "year_tensor = tf.feature_column.input_layer(year,[year_bucket])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([year_tensor]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical_column_with_identity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.]], dtype=float32)]\n",
      "_IdentityCategoricalColumn(key='cat', num_buckets=4, default_value=None)\n"
     ]
    }
   ],
   "source": [
    "# 数字 返回分类后的类别 index \n",
    "# tf.feature_column.categorical_column_with_identity(\n",
    "#     key,\n",
    "#     num_buckets,\n",
    "#     default_value=None\n",
    "# )\n",
    "test = {'cat': [1,3,2,0,3]}\n",
    "# _IdentityCategoricalColumn(key='cat', num_buckets=4, default_value=None)\n",
    "# 类似与 bucketized，不过 bucketized = categorical_column_with_identity + indicator_column\n",
    "column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='cat',\n",
    "    num_buckets=4)\n",
    "\n",
    "indicator = tf.feature_column.indicator_column(column)\n",
    "tensor = tf.feature_column.input_layer(test, [indicator])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(session.run([tensor]))\n",
    "    print(column)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical_column_with_vocabulary_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 0., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)]\n",
      "_VocabularyListCategoricalColumn(key='test', vocabulary_list=('kitchenware', 'electronics', 'sports'), dtype=tf.string, default_value=-1, num_oov_buckets=0)\n"
     ]
    }
   ],
   "source": [
    "# 字符串 返回分类后的类别 index\n",
    "# tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "#     key,\n",
    "#     vocabulary_list,\n",
    "#     dtype=None,\n",
    "#     default_value=-1,\n",
    "#     num_oov_buckets=0\n",
    "# )\n",
    "test = {\"test\": [\"kitchenware\", \"electronics\", \"sport\", \"shirt\"]}\n",
    "# _VocabularyListCategoricalColumn(key='test', vocabulary_list=('kitchenware', 'electronics', 'sports'), dtype=tf.string, default_value=-1, num_oov_buckets=0)\n",
    "vocabulary_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=\"test\",\n",
    "        vocabulary_list=[\"kitchenware\", \"electronics\", \"sports\"])\n",
    "\n",
    "# indicator_column 输入必须是 categorical_column\n",
    "indicator = tf.feature_column.indicator_column(vocabulary_feature_column)\n",
    "\n",
    "tensor = tf.feature_column.input_layer(test,[indicator])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    print(sess.run([tensor]))\n",
    "    print(vocabulary_feature_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical_column_with_vocabulary_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 功能与 categorical_column_with_vocabulary_list相同，不过当 vocabulary_list 太长时，可以写在文件中\n",
    "# categorical_column_with_vocabulary_file 用文件代替，文件每行包含一个 word\n",
    "\n",
    "# tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "#     key,\n",
    "#     vocabulary_file,\n",
    "#     vocabulary_size=None,\n",
    "#     dtype=tf.dtypes.string,\n",
    "#     default_value=None,\n",
    "#     num_oov_buckets=0\n",
    "# )\n",
    "vocabulary_feature_column =tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        key=\"test\",\n",
    "        vocabulary_file=\"product_class.txt\",\n",
    "        vocabulary_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  categorical_column_with_hash_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0.]], dtype=float32)]\n",
      "_HashedCategoricalColumn(key='chars', hash_bucket_size=5, dtype=tf.string)\n"
     ]
    }
   ],
   "source": [
    "# 将字符串直接分成几类\n",
    "# 与 categorical_column_with_identity 类似，只是把数字换成字符串\n",
    "\n",
    "# tf.feature_column.categorical_column_with_hash_bucket(\n",
    "#     key,\n",
    "#     hash_bucket_size,\n",
    "#     dtype=tf.dtypes.string\n",
    "# )\n",
    "test = {'chars': ['a','c','b','d','e','f','g','b']}\n",
    "# _HashedCategoricalColumn(key='chars', hash_bucket_size=5, dtype=tf.string)\n",
    "hash_bucket = tf.feature_column.categorical_column_with_hash_bucket(key = 'chars',hash_bucket_size=5)\n",
    "\n",
    "indicator = tf.feature_column.indicator_column(hash_bucket)\n",
    "tensor = tf.feature_column.input_layer(test,[indicator])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run([tensor]))\n",
    "    print(hash_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crossed_column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "_CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='longtitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(33, 36)), _BucketizedColumn(source_column=_NumericColumn(key='latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(33, 36))), hash_bucket_size=12, hash_key=None)\n"
     ]
    }
   ],
   "source": [
    "# 组合特征构造成整形特征值\n",
    "# tf.feature_column.crossed_column(\n",
    "#     keys,\n",
    "#     hash_bucket_size,\n",
    "#     hash_key=None\n",
    "# )\n",
    "featrues = {\n",
    "        'longtitude': [19,61,30,9,45],\n",
    "        'latitude': [45,40,72,81,24]\n",
    "    }\n",
    "\n",
    "longtitude = tf.feature_column.numeric_column(key = 'longtitude')\n",
    "latitude = tf.feature_column.numeric_column(key = 'latitude')\n",
    "longtitude_bucket = tf.feature_column.bucketized_column(longtitude,[33,36])\n",
    "latitude_bucket = tf.feature_column.bucketized_column(latitude,[33,36])\n",
    "# _CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='longtitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(33, 36)), _BucketizedColumn(source_column=_NumericColumn(key='latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(33, 36))), hash_bucket_size=12, hash_key=None)\n",
    "cross_column = tf.feature_column.crossed_column([longtitude_bucket,latitude_bucket],12)\n",
    "\n",
    "indicator = tf.feature_column.indicator_column(cross_column)\n",
    "tensor = tf.feature_column.input_layer(featrues,[indicator])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    print(sess.run(tensor))\n",
    "    print(cross_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.48584163,  0.40583292,  0.11255753],\n",
      "       [-0.602625  ,  0.05574511, -0.08122382],\n",
      "       [ 0.88433486, -0.22031519,  0.00225582],\n",
      "       [-0.40975562,  0.37421274,  0.68637836],\n",
      "       [-0.13163354,  0.20485495, -0.23570992]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# embedding：最佳值要训练出来，而不是直接求值\n",
    "# tf.feature_column.embedding_column(\n",
    "#     categorical_column,\n",
    "#     dimension,\n",
    "#     combiner='mean',\n",
    "#     initializer=None,\n",
    "#     ckpt_to_load_from=None,\n",
    "#     tensor_name_in_ckpt=None,\n",
    "#     max_norm=None,\n",
    "#     trainable=True\n",
    "# )\n",
    "features = {'pets': ['dog','cat','rabbit','pig','mouse']}  \n",
    "\n",
    "pets_f_c = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'pets',\n",
    "    ['cat','dog','rabbit','pig','mouse'], \n",
    "    dtype=tf.string, \n",
    "    default_value=-1)\n",
    "\n",
    "column = tf.feature_column.embedding_column(pets_f_c, 3)\n",
    "tensor = tf.feature_column.input_layer(features, [column])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    print(session.run([tensor]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.nn.embedding_lookup_sparse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考 https://www.jianshu.com/p/4a7525c018b2\n",
    "当一个特征是多维离散值时，比如一个人喜欢的球员(肯定不止一个)，该如何处理呢？一种方式是每维用 one-hot 表示，加和取平均 one-hot 值。当然也可以用 Embedding 来表示，更能体现特征关联性。\"embedding_lookup_sparse\" 字面意思就是从 sparse 查找出对应的 embedding 表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "WARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\n",
      "[array([[-0.20015812,  0.15428694, -0.38727465],\n",
      "       [-0.7556567 , -0.00486706, -1.3744396 ],\n",
      "       [ 0.1395809 ,  0.7680633 , -0.42304695]], dtype=float32)]\n",
      "[SparseTensorValue(indices=array([[0, 0],\n",
      "       [0, 1],\n",
      "       [0, 2],\n",
      "       [1, 0],\n",
      "       [1, 1],\n",
      "       [1, 2],\n",
      "       [2, 0],\n",
      "       [2, 1]], dtype=int64), values=array([0, 1, 2, 6, 0, 3, 4, 5], dtype=int64), dense_shape=array([3, 3], dtype=int64))]\n"
     ]
    }
   ],
   "source": [
    "# 3个用户一个特征值\n",
    "csv = [\n",
    "  \"1,harden|james|curry\",\n",
    "  \"2,wrestbrook|harden|durant\",\n",
    "  \"3,|paul|towns\",\n",
    "]\n",
    "TAG_SET = [\"harden\", \"james\", \"curry\", \"durant\", \"paul\",\"towns\",\"wrestbrook\"]\n",
    "\n",
    "def sparse_from_csv(csv):\n",
    "    ids,post_tag_strs = tf.decode_csv(csv,[[-1],[\"\"]])\n",
    "    table = tf.contrib.lookup.index_table_from_tensor(mapping=TAG_SET, default_value=-1)\n",
    "    split_tags = tf.string_split(post_tag_strs,\"|\")\n",
    "    return tf.SparseTensor(\n",
    "      indices=split_tags.indices,\n",
    "      values=table.lookup(split_tags.values), ## 这里给出了不同值通过表查到的index ##\n",
    "      dense_shape=split_tags.dense_shape)\n",
    "\n",
    "TAG_EMBEDDING_DIM = 3\n",
    "embedding_params = tf.Variable(tf.truncated_normal([len(TAG_SET), TAG_EMBEDDING_DIM]))\n",
    "\n",
    "tags = sparse_from_csv(csv)\n",
    "embedding = tf.nn.embedding_lookup_sparse(embedding_params,sp_ids=tags,sp_weights=None)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([tf.global_variables_initializer(),tf.tables_initializer()])\n",
    "    print(sess.run([embedding]))\n",
    "    print(sess.run([tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
